---
title: "Outlier detection and removal in Persson 2021 phonetic vowel database"
author: "A Persson & F Jaeger"
date: "\today"
output:
  html_document:
    toc: yes
    toc_depth: '4'
    df_print: paged
  fontsize: 10pt    
  pdf_document:
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
header-includes:
- \usepackage{booktabs}
- \usepackage{siunitx}
- \usepackage{tabto}
- \usepackage{soul}
- \usepackage{xcolor}
- \usepackage{placeins}
- \usepackage{lscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
- \setstcolor{red}
- \usepackage{sectsty}
- \sectionfont{\color{blue}}
- \subsectionfont{\color{blue}}
- \subsubsectionfont{\color{darkgray}}
geometry: margin=2cm
---

```{r set-options, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
library(knitr)

# Set knit defaults for code chunks
opts_chunk$set(
  dev = 'png', # default format of figures
  comment="", 
  echo=FALSE, warning=FALSE, message=FALSE,
  cache=FALSE, 
  size="footnotesize",
  tidy.opts = list(width.cutoff = 200),
  fig.width = 8, fig.height = 4.5, fig.align = "center")

# some useful formatting functions for output of knitting
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, include=FALSE}
# instead of loading the psych library, I'm importing the geometric.mean function
# (otherwise %+% overrides ggplot2's %+%)
geometric.mean = psych::geometric.mean

library(tidyverse)  # data wrangling and plotting
library(plotly)     # for interactive HTML plots
library(mvtnorm)    # multivariate Gaussian distributions
library(magrittr)
library(dplyr)
library(linguisticsdown)  # for inserting IPA symbols
```

```{r constants, include=FALSE}
theme_set(theme_bw())
```

```{r functions}
source("../constants.R")
```

# Overview
This document identifies, corrects and removes outliers in the SwehVd phonetic vowel database. It furthermore adds information to the database and corrects variable names. The output of this document is csv-files with corrected vowel data.

```{r load-data}
# Load the native talker data
d.vowels.swe <- rbind(
  read.csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/raw-formant-data/vowel_formants_swe_summary_native.txt', sep = "\t", fileEncoding = "UTF-16BE"),
  read.csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/raw-formant-data/vowel_formants_swe_summary_SW043.txt', sep = "\t", fileEncoding = "UTF-16BE"))%>%
  mutate(
    Trial = as.numeric(gsub("^.*\\_([0-9]+)$", "\\1", Token)),
    F0.mean = ifelse(F0.mean == "--undefined--", NA, as.numeric(F0.mean)),
    Vowel = factor(
      plyr::mapvalues(
        Vowel,
        levels.vowel.SweFA,
        levels.vowel.IPA.swe),
      levels = levels.vowel.IPA.swe)) %>%
  rename(
    Talker = Filename,
    Duration = "Duration..s.",
    F0 = F0.mean) %>%
  # obtain SR %>%
  group_by(Talker) %>%
  mutate(F0_gm = geometric.mean(F0)) %>%
  ungroup() %>%
  mutate(SR = F0_to_SR(F0_gm)) %>%
  # create unique identifier for each vowel token
  group_by(Talker, Vowel) %>%
  mutate(Token = factor(as.numeric(as.factor(Trial))),
         Quantity = case_when(Vowel %in% levels.vowel.IPA.swe.long ~ "long",
                              Vowel %in% levels.vowel.IPA.swe.short ~"short"))

```

# Repairing variables
We add information on talker age (collected manually on paper forms when recording).

```{r}
d.vowels.swe %<>%
  mutate(
    Age = case_when(Talker == "SW_001" ~ 30,
                    Talker == "SW_002" ~ 29,
                    Talker == "SW_003" ~ 29,
                    Talker == "SW_004" ~ 26,
                    Talker == "SW_005" ~ 29,
                    Talker == "SW_006" ~ 38,
                    Talker == "SW_007" ~ 32,
                    Talker == "SW_008" ~ 36,
                    Talker == "SW_009" ~ 33,
                    Talker == "SW_010" ~ 22,
                    Talker == "SW_011" ~ 44,
                    Talker == "SW_012" ~ 22,
                    Talker == "SW_013" ~ 25,
                    Talker == "SW_014" ~ 28,
                    Talker == "SW_015" ~ 25,
                    Talker == "SW_016" ~ 26,
                    Talker == "SW_017" ~ 24,
                    Talker == "SW_018" ~ 28,
                    Talker == "SW_019" ~ 26,
                    Talker == "SW_020" ~ 22,
                    Talker == "SW_021" ~ 20,
                    Talker == "SW_022" ~ 32,
                    Talker == "SW_023" ~ 32,
                    Talker == "SW_024" ~ 22,
                    Talker == "SW_043" ~ 35)) %>%
  relocate(Age, .after = Talker)
```

Some vowels were misidentified/misproduced by the talkers. This was e.g. the case for several of the /hodd/ tokens. After listening through all sound files, we identified these tokens and add a column to the corrected data. In column 'Transcribed vowel', the correctly produced vowels are labelled 'targeted', the mispronounced vowels are labelled with their corresponding IPA symbol (as defined by Anna Persson).

Some talkers only did 5 recordings of each vowel in a first session. They were called back, some returned but some did not. We add a column indicating the number of recordings per talker.

```{r}
d.vowels.swe %<>%
  mutate(
    Transcribed_vowel = case_when(Talker == "SW_001" & Vowel == "[ʊ]" & Token == 1 ~ "[ɔ]",
                                  Talker == "SW_003" & Vowel == "[ʊ]" & Token %in% c(1, 5)  ~ "[ɔ]",
                                  Talker == "SW_004" & Vowel == "[uː]" & Token %in% c(1, 2)  ~ "[ɔ]",
                                  Talker == "SW_004" & Vowel == "[ʊ]" & Token == 1  ~ "[ɔ]",
                                  Talker == "SW_005" & Vowel == "[uː]" & Token == 1  ~ "[ɔ]",
                                  Talker == "SW_005" & Vowel == "[ʊ]" & Token == 1  ~ "[ɔ]",
                                  Talker == "SW_008" & Vowel == "[ʊ]"  ~ "[ɔ]",
                                  Talker == "SW_009" & Vowel == "[ʊ]"  ~ "[ɔ]",
                                  Talker == "SW_010" & Vowel == "[ʊ]"  ~ "[ɔ]",
                                  Talker == "SW_011" & Vowel == "[uː]" & Token == 1  ~ "[ɔ]",
                                  Talker == "SW_012" & Vowel == "[ʊ]" & Token %in% c(1, 7)  ~ "[ɔ]",
                                  Talker == "SW_014" & Vowel == "[ʊ]" & Token %in% c(1,2,3,4,7,8)  ~ "[ɔ]",
                                  Talker == "SW_018" & Vowel == "[ʊ]" & Token != 10  ~ "[ɔ]",
                                  Talker == "SW_020" & Vowel == "[ʊ]" & Token == 1  ~ "[ɔ]",
                                  Talker == "SW_021" & Vowel == "[ʊ]" & Token == 1  ~ "[ɔ]",
                                  Talker == "SW_021" & Vowel == "[aː]" & Token == 7  ~ "[æ]",
                                  Talker == "SW_024" & Vowel == "[uː]" & Token %in% c(1, 6)  ~ "[ɔ]",
                                  Talker == "SW_024" & Vowel == "[ʊ]"  ~ "[ɔ]",
                                  TRUE ~ "targeted"))
```

# Outlier detection

The following plots shows the distribution of the automatically extracted F1 and F2 values across vowels and talkers. These visualizations strongly suggest that the automatic formant extraction resulted in outliers due to, for example, measurement errors.

```{r vowel-distribution-uncorrected}
d.vowels.swe %<>%
  # focus on F1-F2
  select(
    Talker, Age, Vowel, Transcribed_vowel, Word, Token, Trial, F0_gm, F0, Duration, SR, Quantity,
    starts_with("F1"), starts_with("F2"), starts_with("F3")) %>%
  # have the 3 measures for each formant be in different rows, rather than columns
  pivot_longer(
    cols = c(
      starts_with("F1"),
      starts_with("F2"),
      starts_with("F3")),
    names_to = c("Formant", "Location"),
    names_pattern = "(F.)\\_(.[0-9]*)") %>%
  pivot_wider(
    names_from = "Formant",
    values_from = "value"
  ) %>%
  mutate(
    height = F1_to_height(F1, SR),
    backness = F2_to_backness(F1, F2))

p = d.vowels.swe %>%
# Plot the long/short vowels separately
    filter(Quantity == "long") %>%
  ggplot(
    aes(
      x = F2, 
      y = F1,
      color = Vowel,
      shape = Location)) +
  scale_colour_manual(name = "Vowel",values = colors.vowel.swe) +
  geom_point(alpha = .5) + 
  scale_x_reverse() +
  scale_y_reverse() +
  scale_alpha_discrete("Outlier") +
  facet_wrap(~ Talker)
plot(p)
```


```{r}
p %+%
  (d.vowels.swe %>%
     filter(Quantity == "short"))

```
One common way to detect outliers is scaling, i.e., centering the data and then dividing it by its standard deviation. However, scaling changes the correlations between variables. For the present case, scaling would change the correlations between the formant values. This might introduce artifacts since we are interested in detecting productions that are outliers *relative to the joint distribution of formant values*. In particular, we are interested in the joint distribution of F1 and F2. 

```{r set-outlier-cutoff, include=F}
# What proportion of the most extreme values should be considered outliers? 
# (if set to e.g., .05 that means that points with cumulative densities below
# .025 or above .975 are considered outliers)
outlier_probability_cutoff = .01
```

We took a different approach, estimating the joint bivariate distribution along F1-F2 separately for each unique combination of vowel and talker. This allowed us to estimate, for each observation of F1-F2, the relative probability of observing a value as extreme as this given the joint distribution of F1-F2 for that vowel and talker. 
We also estimated the joint multivariate distribution along all cues (F0, F1, F2, F3 and vowel duration). This was compared against 5 separate univariate distributions to assess whether the number of identified outliers was driven by one or a couple of the five cues.

First show the density of the F0s prior to measurement correction.

```{r}
  d.vowels.swe %>% 
  # Get the geometric mean across all five time points for the five cues
  group_by(Talker, Vowel, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(F0)) +
  geom_density(
    mapping = aes(fill = Vowel, linetype = Quantity), 
    alpha = .3, position = "identity") +
  facet_wrap(~Quantity)
```



```{r }
## CANNOT IDENTIFY OUTLIERS WITH MISSING VALUES IN DATA, THEREFORE, filter out missing F0s first, they are corrected further down
d.outliers.univariate <- d.vowels.swe %>%
  filter(!is.na(F0)) %>%
  obtain_densities_univariates() %>%
  select(Talker, Vowel, Token, Trial, Location, Quantity, F0, F1, F2, F3, Duration, starts_with("cumulative"))

#Visualize univariate outliers
 p.outliers.univariate.F1 <- d.outliers.univariate %>%
  #filter(Quantity == "long") %>%
  ggplot(
    aes(
      x = F2,
      y = F1,
      color = Vowel,
      shape = Location,
      alpha = is_outlier(cumulative_probability_F1))) +
  scale_colour_manual(name = "Vowel",values = colors.vowel.swe) +
  geom_point() +
  scale_x_reverse() +
  scale_y_reverse() +
  facet_wrap(~Quantity) +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5))

p.outliers.univariate.F2 <- p.outliers.univariate.F1 %+%
  aes(alpha = is_outlier(cumulative_probability_F2))

p.outliers.univariate.F3 <- p.outliers.univariate.F1 %+%
  aes(
    x = F2,
    y = F3,
    alpha = is_outlier(cumulative_probability_F3))

p.outliers.univariate.F0 <- p.outliers.univariate.F1 %+%
  aes(
    x = F2,
    y = F0,
    alpha = is_outlier(cumulative_probability_F0))

p.outliers.univariate.duration <- p.outliers.univariate.F1 %+%
  aes(
    x = Duration,
    y = F1,
    alpha = is_outlier(cumulative_probability_Duration)) +
  scale_x_continuous("Duration")

```


```{r }
d.outliers.multivariate <- d.vowels.swe %>%
  filter(!is.na(F0)) %>%
  obtain_densities_allCues() %>%
  select(Talker, Vowel, Token, Trial, Location, Quantity, F0, F1, F2, F3, Duration, starts_with("cumulative"))

p.outliers.multivariate <- p.outliers.univariate.F1 %+%
  (d.outliers.multivariate) +
  aes(alpha = is_outlier(cumulative_probability_allCues))

cowplot::plot_grid(p.outliers.univariate.F0, p.outliers.univariate.F1, p.outliers.univariate.F2, p.outliers.univariate.F3, p.outliers.univariate.duration, p.outliers.multivariate, ncol = 2, labels = c("F0 outliers", "F1 outliers", "F2 outliers", "F3 outliers", "Duration outliers", "multivariate outliers"))
```


```{r }
d.outliers.multivariate %<>%
  filter(!between(cumulative_probability_allCues, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
  rename(cumulative_probability = cumulative_probability_allCues) %>%
  mutate(outlier_source = "cues_multivariate")

d.vowels.repair <- rbind(
  #First get the F0 outliers
      d.outliers.univariate %>%
        select(-c(cumulative_probability_F1, cumulative_probability_F2, cumulative_probability_F3, cumulative_probability_Duration)) %>%
        filter(!between(cumulative_probability_F0, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
        rename(cumulative_probability = cumulative_probability_F0) %>%
        mutate(outlier_source = "F0_univariate"),
      #Then get the F1 outliers
      d.outliers.univariate %>%
        select(-c(cumulative_probability_F0, cumulative_probability_F2, cumulative_probability_F3, cumulative_probability_Duration)) %>%
        filter(!between(cumulative_probability_F1, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
        rename(cumulative_probability = cumulative_probability_F1) %>%
        mutate(outlier_source = "F1_univariate"),
        #Then get the F2 outliers
      d.outliers.univariate %>%
        select(-c(cumulative_probability_F0, cumulative_probability_F1, cumulative_probability_F3, cumulative_probability_Duration)) %>%
        filter(!between(cumulative_probability_F2, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
        rename(cumulative_probability = cumulative_probability_F2) %>%
        mutate(outlier_source = "F2_univariate"),
        #Then get the F3 outliers
      d.outliers.univariate %>%
        select(-c(cumulative_probability_F0, cumulative_probability_F1, cumulative_probability_F2, cumulative_probability_Duration)) %>%
        filter(!between(cumulative_probability_F3, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
        rename(cumulative_probability = cumulative_probability_F3) %>%
        mutate(outlier_source = "F3_univariate"),
        #Then get the Duration outliers
      d.outliers.univariate %>%
        select(-c(cumulative_probability_F0, cumulative_probability_F1, cumulative_probability_F2, cumulative_probability_F3)) %>%
        filter(!between(cumulative_probability_Duration, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
        rename(cumulative_probability = cumulative_probability_Duration) %>%
        mutate(outlier_source = "Duration_univariate")) %>%
  left_join(
    d.outliers.multivariate,
    by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity", "F0", "F1", "F2", "F3", "Duration")) %>%
  mutate(
    cumulative_probability = ifelse(is.na(cumulative_probability.x), cumulative_probability.y, cumulative_probability.x),
    outlier_source = ifelse(is.na(outlier_source.x), outlier_source.y, outlier_source.x)) %>%
  select(-c(cumulative_probability.x, outlier_source.x, cumulative_probability.y, outlier_source.y))

#Evaluate the no of tokens to repair based on each cue
d.vowels.repair %>%
  group_by(outlier_source) %>%
  tally()
```

# Outlier correction
We manually corrected all outliers with a normalized density of less than .05. We first checked the segmentation boundaries in the Praat textgrid. Next, we manually measured new formant values using visual approximation of time points and Praat's function *Formant: Formant listing*. When the formant value generated diverged substantially from that talker's general value for that time point and that vowel, and/or the formant track did not follow a stable path in the spectrogram, the spectrogram was read manually by clicking the formant and reading off the value at the horizontal line. We then imported the corrected formant data for the outliers to overwrite the automatically obtained formants.

```{r message=FALSE}
# This code was originally run in order to generate a csv file with all cases that require manual correction. 
# (kept here for reproducibility)
#
# d.vowels.swe_repair <- d.vowels.swe %>%
#   select(Talker, Vowel, Token, Trial, Location, Quantity, F1, F2, cumulative_probability_F1F2) %>%
#   filter(!between(cumulative_probability_F1F2, outlier_probability_cutoff, 1 - outlier_probability_cutoff))

# write.csv(
#   d.vowels.swe_repair %>%
#     # Mutate back to arpabet for csv readability
#     mutate(Vowel = factor(
#       plyr::mapvalues(
#         Vowel,
#         levels.vowel.IPA.swe,
#         levels.vowel.SweFA),
#       levels = levels.vowel.SweFA)),
#   file = "../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_vowels_to_repair.csv",
#   row.names = FALSE, quote = FALSE)

# Write an additional file to repair with all cues BUT PRIOR TO WRITING ANOTHER REPAIR FILE, CHECK WHETHER THE ALREADY REPAIRED DATA IS IN THERE.
d.vowels.repair %<>%
  anti_join(read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_vowels_repaired.csv') %>%
              select(Talker, Vowel, Token, Trial, Location, F1, F2, Quantity) %>%
              mutate(
                Location = gsub("\\_", "", Location),
                Vowel = factor(
                  plyr::mapvalues(
                    Vowel,
                    levels.vowel.SweFA,
                    levels.vowel.IPA.swe),
                  levels = levels.vowel.IPA.swe)) %>%
              mutate_at("Token", factor),
            by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity"))

d.vowels.repair %<>% 
  anti_join(read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_repaired.csv') %>%
              select(Talker, Vowel, Token, Trial, Location, F0.mean, Quantity) %>%
              rename(F0 = F0.mean) %>%
              mutate(
                Location = gsub("\\_", "", Location),
                Vowel = factor(
                  plyr::mapvalues(
                    Vowel,
                    levels.vowel.SweFA,
                    levels.vowel.IPA.swe),
                  levels = levels.vowel.IPA.swe)) %>%
              mutate_at("Token", factor),
              by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity")) %>%
  arrange(Talker, Vowel, Token, Location, Quantity, F0, F1, F2, F3, Duration)

#CODE KEPT FOR REPRODUCIBILITY
# write.csv(
#   d.vowels.repair %>% 
#     # Mutate back to arpabet for csv readability
#     mutate(Vowel = factor(
#       plyr::mapvalues(
#         Vowel,
#         levels.vowel.IPA.swe,
#         levels.vowel.SweFA),
#       levels = levels.vowel.SweFA),
#       cumulative_probability = as.numeric(cumulative_probability)),
#     file = "../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_allCuesvowels_to_repair.csv", row.names = FALSE, quote = FALSE)
```


```{r vowel-correction, message=FALSE}
# Read in csv-file with manually repaired F0s and duration values and join it to vowel data.
#BEGIN WITH THE MOST RECENT FILE THAT IS INCOMPLETE AND CURRENTLY ONLY CONTAINS CORRECTED F0 AND DURATION
d.vowels.swe %<>%
  select(Talker, Age, Vowel, Transcribed_vowel, Word, Token, Trial, Location, F1, F2, F3, F0_gm, F0, Duration, SR, Quantity) %>%
  left_join(
  read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_allCuesvowels_repairedF0Dur.csv') %>%
      select(Talker, Vowel, Token, Trial, Location, Quantity, F0, Duration, outlier_source) %>%
      filter(outlier_source %in% c("F0_univariate", "Duration_univariate")) %>%
      mutate(
        Location = gsub("\\_", "", Location),
        Vowel = factor(
      plyr::mapvalues(
        Vowel,
        levels.vowel.SweFA,
        levels.vowel.IPA.swe),
      levels = levels.vowel.IPA.swe)) %>%
      mutate_at("Token", factor),
  #The correction file sometimes did not output F0 and Duration values for all time points, since they are values across the vowel, we overwrite the missing rows by joining without Location
    by = c("Talker", "Vowel", "Token", "Trial", "Quantity")) %>%
  # whenever there is a corrected F0 or duration value (F0.y, Duration.y), use that value; otherwise use the
  # automatically-extracted F0-Duration value (F0.x, Duration.x).
  mutate(
    F0 = ifelse(!is.na(F0.y), F0.y, F0.x),
    Duration = ifelse(!is.na(Duration.y), Duration.y, Duration.x),
    Location = Location.x) %>%
  select(-c(F0.x, F0.y, Duration.x, Duration.y, Location.x, Location.y)) %>%
  select(Talker, Age, Vowel, Transcribed_vowel,  Word, Token, Trial, Location, F0_gm, F0, Duration, SR, F1, F2, F3, Quantity) %>%
  #remove duplicate rows
  unique()
  
# Read in previously corrected F1 and F2 values, based on a bivariate probability distribution (see further up)
d.vowels.swe %<>%
  select(Talker, Age, Vowel, Transcribed_vowel, Word, Token, Trial, Location, F1, F2, F3, F0_gm, F0, Duration, SR, Quantity) %>%
  # add the corrected F1, F2
  left_join(
    read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_vowels_repaired.csv') %>%
      select(Talker, Vowel, Token, Trial, Location, F1, F2, Quantity) %>%
      mutate(
        Location = gsub("\\_", "", Location),
        Vowel = factor(
      plyr::mapvalues(
        Vowel,
        levels.vowel.SweFA,
        levels.vowel.IPA.swe),
      levels = levels.vowel.IPA.swe)) %>%
      mutate_at("Token", factor),
    by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity")) %>%
  # whenever there is a corrected F1-F2 value (F1.y, F2.y), use that value; otherwise use the
  # automatically-extracted F1-F2 value (F1.x, F2.x).
  mutate(
    F1 = ifelse(!is.na(F1.y), F1.y, F1.x),
    F2 = ifelse(!is.na(F2.y), F2.y, F2.x),
    height = F1_to_height(F1, SR),
    backness = F2_to_backness(F1, F2)) %>%
  select(-c(F1.x, F2.x, F1.y, F2.y)) %>%
  select(Talker, Age, Vowel, Transcribed_vowel,  Word, Token, Trial, Location, F0_gm, F0, Duration, SR, F1, F2, F3, height, backness, Quantity)

```

## Missing F0 values
There were some missing F0 values in the automatically extracted formant data. There were also halving/doubling instances. Here, we write a file with missing values and values that might be an instance of halving/doubling. These values are manually measured in Praat.
Most of the cases with NAs concern talkers with creaky voices. When this was the case, we set the default setting for extracting pitch in praat to 40 Hz as lower boundary, and 500 Hz as upper. When this was not the case, we set the lower boundary to 100 for female voices. When a value could not be reliably estimated, we left the value as NA.

```{r}
#CODE KEPT FOR REPRODUCIBILITY
# write.csv(
#   d.vowels.swe %>%
#     # Mutate back to arpabet for csv readability
#     mutate(Vowel = factor(
#       plyr::mapvalues(
#         Vowel,
#         levels.vowel.IPA.swe,
#         levels.vowel.SweFA),
#       levels = levels.vowel.SweFA)) %>%
#     #rename(f0gm = F0_gm, f0m = F0.mean) %>%
#     group_by(Talker, Vowel) %>%
#     mutate(
#       F0_half = ifelse(F0 < (F0_gm / 2), TRUE, FALSE),
#       F0_double = ifelse(F0 > (F0_gm * 2), TRUE, FALSE)) %>%
#     filter(F0_half == TRUE | F0_double == TRUE | is.na(F0)) %>%
#     select(Talker, Vowel, Token, Trial, Location, Quantity, F1, F2, F0),
#   file = "../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_to_repair.csv",
#   row.names = FALSE, quote = FALSE)

#Read in file with manually extracted F0s
d.vowels.swe %<>%
  mutate(
   # Introduce column for unreliable measurements, to be overwritten further down
   Unreliable_measurement = NA) %>%
  # add the corrected F0s
  left_join(
    read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_repaired.csv') %>%
      select(Talker, Vowel, Token, Trial, Location, Quantity, F0.mean) %>%
      #Rename since labelling has changed
      rename(F0 = F0.mean) %>%
      mutate(
        Location = gsub("\\_", "", Location),
        Vowel = factor(
          plyr::mapvalues(
            Vowel,
            levels.vowel.SweFA,
            levels.vowel.IPA.swe),
          levels = levels.vowel.IPA.swe),
        # Introduce column for unreliable measurements
        Unreliable_measurement = case_when(
          Talker %in% c("SW_003", "SW_005", "SW_008", "SW_011", "SW_017", "SW_018", "SW_021", "SW_022", "SW_024") & !is.na(F0) ~ "creaky voice",
          Talker == "SW_007" & Vowel == "ee1" ~ "creaky voice",
          Talker == "SW_008" & Vowel == "aa1" ~ "TRUE",
          is.na(F0) ~ "TRUE",
                TRUE ~ "FALSE")) %>%
      mutate_at("Token", factor),
    by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity")) %>%
  # whenever there is a corrected value (.y, .y), use that value; otherwise use the
  # automatically-extracted value (.x, .x).
  mutate(
    F0 = ifelse(!is.na(F0.y), F0.y, F0.x),
    Unreliable_measurement = ifelse(!is.na(Unreliable_measurement.y), Unreliable_measurement.y, Unreliable_measurement.x)) %>%
  select(-c(F0.x, F0.y, Unreliable_measurement.x, Unreliable_measurement.y)) %>%
  mutate(
    Unreliable_measurement = ifelse(is.na(Unreliable_measurement), "FALSE", Unreliable_measurement))
```

Now plot the distribution of F0 again.

```{r}
d.vowels.swe %>% 
  # Get the geometric mean across all five time points for the five cues
  group_by(Talker, Vowel, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(F0)) +
  geom_density(
    mapping = aes(fill = Vowel, linetype = Quantity), 
    alpha = .3, position = "identity") +
  facet_wrap(~Quantity)
```

Do another round of univariate F0s to correct.

```{r}
d.outliers.univariateF0.no2 <- d.vowels.swe %>%
  obtain_densities_univariates() %>%
  select(Talker, Vowel, Token, Trial, Location, Quantity, F0, F1, F2, F3, Duration, cumulative_probability_F0) %>%
  filter(!between(cumulative_probability_F0, outlier_probability_cutoff, 1 - outlier_probability_cutoff)) %>%
  rename(cumulative_probability = cumulative_probability_F0) %>%
  mutate(outlier_source = "F0_univariate")

#CODE KEPT FOR REPRODUCIBILITY
# write.csv(
#   d.outliers.univariateF0.no2 %>%
#     # Mutate back to arpabet for csv readability
#     mutate(Vowel = factor(
#       plyr::mapvalues(
#         Vowel,
#         levels.vowel.IPA.swe,
#         levels.vowel.SweFA),
#       levels = levels.vowel.SweFA),
#       cumulative_probability = as.numeric(cumulative_probability)),
#     file = "../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_to_repair_no2.csv", row.names = FALSE, quote = FALSE)

#Read in corrected values
d.vowels.swe %<>%
  select(Talker, Age, Vowel, Transcribed_vowel, Word, Token, Trial, Location, F1, F2, F3, F0_gm, F0, Duration, SR, Quantity, Unreliable_measurement) %>%
  left_join(
  read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_no2_repaired.csv') %>%
      select(Talker, Vowel, Token, Trial, Location, Quantity, F0, Duration, outlier_source) %>%
      mutate(
        Location = gsub("\\_", "", Location),
        Vowel = factor(
      plyr::mapvalues(
        Vowel,
        levels.vowel.SweFA,
        levels.vowel.IPA.swe),
      levels = levels.vowel.IPA.swe)) %>%
      mutate_at("Token", factor),
  #The correction file sometimes did not output F0 and Duration values for all time points, since they are values across the vowel, we overwrite the missing rows by joining without Location
    by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity")) %>%
  # whenever there is a corrected F0 or duration value (F0.y, Duration.y), use that value; otherwise use the
  # automatically-extracted F0-Duration value (F0.x, Duration.x).
  mutate(
    F0 = ifelse(!is.na(F0.y), F0.y, F0.x),
    Duration = ifelse(!is.na(Duration.y), Duration.y, Duration.x)) %>%
  select(-c(F0.x, F0.y, Duration.x, Duration.y)) %>%
  select(Talker, Age, Vowel, Transcribed_vowel,  Word, Token, Trial, Location, F0_gm, F0, Duration, SR, F1, F2, F3, Quantity, Unreliable_measurement)

#And plot again
d.vowels.swe %>% 
  # Get the geometric mean across all five time points for the five cues
  group_by(Talker, Vowel, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(F0)) +
  geom_density(
    mapping = aes(fill = Vowel, linetype = Quantity), 
    alpha = .3, position = "identity") +
  facet_wrap(~Quantity)

```

```{r}
d.outliers.univariateF0.no3 <- d.vowels.swe %>%
  obtain_densities_univariates() %>%
  select(Talker, Vowel, Token, Trial, Location, Quantity, F0, F1, F2, F3, Duration, cumulative_probability_F0) %>%
  # To be conservative, we also checked all F0 values going a bit beyond that half-half point
  filter(!between(cumulative_probability_F0, outlier_probability_cutoff, 1 - outlier_probability_cutoff) | F0 < 150) %>%
  rename(cumulative_probability = cumulative_probability_F0) %>%
  mutate(outlier_source = "F0_univariate")

#CODE KEPT FOR REPRODUCIBILITY
# write.csv(
#   d.outliers.univariateF0.no3 %>%
#     # Mutate back to arpabet for csv readability
#     mutate(Vowel = factor(
#       plyr::mapvalues(
#         Vowel,
#         levels.vowel.IPA.swe,
#         levels.vowel.SweFA),
#       levels = levels.vowel.SweFA),
#       cumulative_probability = as.numeric(cumulative_probability)),
#     file = "../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_to_repair_no3.csv", row.names = FALSE, quote = FALSE)
```

```{r}
#Read in corrected values again
d.vowels.swe %<>%
  select(Talker, Age, Vowel, Transcribed_vowel, Word, Token, Trial, Location, F1, F2, F3, F0_gm, F0, Duration, SR, Quantity, Unreliable_measurement) %>%
  left_join(
    read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_F0_no3_repaired.csv') %>%
      select(Talker, Vowel, Token, Trial, Location, Quantity, F0, outlier_source) %>%
      mutate(
        Location = gsub("\\_", "", Location),
        Vowel = factor(
      plyr::mapvalues(
        Vowel,
        levels.vowel.SweFA,
        levels.vowel.IPA.swe),
      levels = levels.vowel.IPA.swe)) %>%
      mutate_at("Token", factor),
  #The correction file sometimes did not output F0 and Duration values for all time points, since they are values across the vowel, we overwrite the missing rows by joining without Location
    by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity")) %>%
  # whenever there is a corrected F0 or duration value (F0.y, Duration.y), use that value; otherwise use the
  # automatically-extracted F0-Duration value (F0.x, Duration.x).
  mutate(
    F0 = ifelse(!is.na(F0.y), F0.y, F0.x)) %>%
  select(-c(F0.x, F0.y)) %>%
  select(Talker, Age, Vowel, Transcribed_vowel,  Word, Token, Trial, Location, F0_gm, F0, Duration, SR, F1, F2, F3, Quantity, Unreliable_measurement)

#And plot again
d.vowels.swe %>% 
  # Get the geometric mean across all five time points for the five cues
  group_by(Talker, Vowel, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(F0)) +
  geom_density(
    mapping = aes(fill = Vowel, linetype = Quantity), 
    alpha = .3, position = "identity") +
  facet_wrap(~Quantity)

#Also, read in the corrected F3-values
d.vowels.swe %<>%
  select(Talker, Age, Vowel, Transcribed_vowel, Word, Token, Trial, Location, F1, F2, F3, F0_gm, F0, Duration, SR, Quantity, Unreliable_measurement) %>%
  # add the corrected F1, F2
  left_join(
    read_csv('../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_repaired_F3.csv') %>%
      select(Talker, Vowel, Token, Trial, Location, F3, Quantity) %>%
      mutate(
        Location = gsub("\\_", "", Location),
        Vowel = factor(
      plyr::mapvalues(
        Vowel,
        levels.vowel.SweFA,
        levels.vowel.IPA.swe),
      levels = levels.vowel.IPA.swe)) %>%
      mutate_at("Token", factor),
    by = c("Talker", "Vowel", "Token", "Trial", "Location", "Quantity")) %>%
  # whenever there is a corrected F3 value (F3.y), use that value; otherwise use the
  # automatically-extracted F3 value (F3.x).
  mutate(
    F3 = ifelse(!is.na(F3.y), F3.y, F3.x)) %>%
  select(-c(F3.x, F3.y)) %>%
  select(Talker, Age, Vowel, Transcribed_vowel,  Word, Token, Trial, Location, F0_gm, F0, Duration, SR, F1, F2, F3, Quantity, Unreliable_measurement)

#Also plot the F3s after correction
d.vowels.swe %>% 
  # Get the geometric mean across all five time points for the five cues
  group_by(Talker, Vowel, Token, Quantity) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  ggplot(
    aes(F3)) +
  geom_density(
    mapping = aes(fill = Vowel, linetype = Quantity), 
    alpha = .3, position = "identity") +
  facet_wrap(~Quantity)

p.outliers.univariate.F3 %+%
  (d.vowels.swe) +
  aes(alpha = .4)
```

## Vowel data *with* distributional outliers

```{r}
d.vowels.swe.wDistrOutliers <- d.vowels.swe %>%
  # Remove Talker SW_006 as she is not born/raised in Stockholm
  filter(Talker != "SW_006")

#CODE KEPT FOR REPRODUCIBILITY
# Write csv-file with the manually corrected vowel data, but without removed outliers.
# write.csv(
#   d.vowels.swe.wDistrOutliers %>%
#     # remove average F0 gm of each talker
#     select(-F0_gm) %>%
#     # Variable renaming
#     rename(category = Vowel) %>%
#     relocate(F0, .before = F1),
#   file = "../../data/phonetic vowel statistics/Published databases/Persson-2021/corrected-vowel-data/Persson_2021_L1_vowels_wDistrOutliers.csv",
#   row.names = FALSE, quote = FALSE)
```
